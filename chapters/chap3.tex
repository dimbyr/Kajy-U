\chapter{Complexité algorithmique}

La complexité algorithmique permet d'évaluer l'efficacité des algorithmes en termes de temps d'exécution et de consommation de ressources. Cela aide à comprendre combien de temps un algorithme prendra pour des entrées de tailles différentes. Les notations couramment utilisées pour décrire la complexité algorithmique sont $O$, $\Omega$, et $\theta$.

\section{Les bornes asymptotiques}

Les bornes asymptotiques, ou "asymptotic bounds," fournissent une approximation de la performance d'un algorithme pour des tailles d'entrée importantes. Elles aident à évaluer le comportement des algorithmes sous différents scénarios : meilleur cas, pire cas, et cas moyen.

\begin{itemize}
	\item \textbf{Borne supérieure (Big O)} : Représentée par la notation \(O(f(n))\), cette borne donne une estimation de la complexité maximale d'un algorithme, indiquant qu'il n'aura jamais une complexité supérieure à \(O(f(n))\). Mathématiquement, cela signifie qu'il existe des constantes \(c\) et \(n_0\) telles que :
	
	\[ T(n) \leq c \cdot f(n), \quad \text{pour tout } n \geq n_0. \]
	
	\item \textbf{Borne inférieure (Big Omega)} : Représentée par la notation \(\Omega(f(n))\), cette borne donne une estimation de la complexité minimale, indiquant que l'algorithme ne peut pas être plus rapide que cette borne. Cela signifie qu'il existe des constantes \(c\) et \(n_0\) telles que :
	
	\[ T(n) \geq c \cdot f(n), \quad \text{pour tout } n \geq n_0. \]
	
	\item \textbf{Borne moyenne (Big Theta)} : Représentée par la notation \(\theta(f(n))\), cette borne indique une complexité moyenne ou attendue. Cela signifie que l'algorithme a une borne supérieure et inférieure qui convergent vers la même fonction. Mathématiquement, cela signifie qu'il existe des constantes \(c_1\), \(c_2\), et \(n_0\) telles que :
	
	\[ c_1 \cdot f(n) \leq T(n) \leq c_2 \cdot f(n), \quad \text{pour tout } n \geq n_0. \]
\end{itemize}

\section{La notation O}

La notation "O" (grand O) indique la complexité maximale d'un algorithme en termes de temps ou d'espace. Elle fournit une estimation du nombre maximum d'opérations nécessaires en fonction de la taille de l'entrée. Par exemple, la recherche binaire a une complexité de `O(log n)`, car chaque étape réduit le problème de moitié.

\begin{algorithm}
	\caption{Recherche binaire}
	\begin{algorithmic}[1]
		\Function{recherche\_binaire}{tableau, valeur}
		\State gauche := 0
		\State droite := taille(tableau) - 1
		\While{gauche <= droite}
		\State milieu := (gauche + droite) / 2
		\If{tableau[milieu] == valeur}
		\State \textbf{Retourner} milieu
		\ElsIf{tableau[milieu] < valeur}
		\State gauche := milieu + 1
		\Else
		\State droite := milieu - 1
		\EndIf
		\EndWhile
		\State \textbf{Retourner} -1
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\section{Les notations $\Omega$ et $\theta$}

Ces notations complètent la notation "O" pour donner une vision complète de la complexité algorithmique.

\begin{itemize}
	\item \textbf{Borne inférieure (\(\Omega\))} : Représente la complexité minimale. Par exemple, la complexité de l'algorithme de somme de tableau est $\Omega(n)$, car chaque élément doit être parcouru.
	
	\item \textbf{Borne moyenne (\(\theta\))} : Indique la complexité moyenne ou attendue, définissant à la fois une borne inférieure et une borne supérieure qui convergent. Pour le même algorithme de somme de tableau, la complexité est $\theta(n)$. 
\end{itemize}

Voici un exemple de la somme des éléments d'un tableau :

\begin{algorithm}
	\caption{Somme de tableau}
	\begin{algorithmic}[1]
		\Function{somme}{tableau}
		\State somme := 0
		\For{i de 0 à taille(tableau) - 1}
		\State somme := somme + tableau[i]
		\EndFor
		\State \textbf{Retourner} somme
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\section{Complexité de certains algorithmes}
Voici quelques exemples d'algorithmes qui illustrent des complexités courantes, allant du temps constant à l'exponentiel.

\subsection{Temps constant (O(1))}

Les algorithmes à temps constant ont une complexité qui ne dépend pas de la taille de l'entrée. Par exemple, accéder à un élément spécifique d'un tableau par son index a une complexité `O(1)`. Cela signifie qu'il y a toujours une seule opération, quelle que soit la taille du tableau.

\begin{algorithm}
	\caption{Accès direct à un élément du tableau}
	\begin{algorithmic}[1]
		\Function{accès\_direct}{tableau, index}
		\State \textbf{Retourner} tableau[index]
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Temps linéaire (O(n))}

Les algorithmes à temps linéaire ont une complexité qui augmente proportionnellement avec la taille de l'entrée. Par exemple, le calcul de la somme des éléments d'un tableau a une complexité `O(n)`, car il faut parcourir tous les éléments.

\begin{algorithm}
	\caption{Calcul de la somme des éléments d'un tableau}
	\begin{algorithmic}[1]
		\Function{somme}{tableau}
		\State somme := 0
		\For{i de 0 à taille(tableau) - 1}
		\State somme := somme + tableau[i]
		\EndFor
		\State \textbf{Retourner} somme
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Temps quadratique (O($n^2$))}

Les algorithmes à temps quadratique ont une complexité qui augmente avec le carré de la taille de l'entrée. Par exemple, le tri par insertion a une complexité `O($n^2$)`, car chaque élément doit être comparé avec tous les éléments précédents.

\begin{algorithm}
	\caption{Tri par insertion}
	\begin{algorithmic}[1]
		\For{i de 1 à n - 1}
		\State clé := tableau[i]
		\State j := i - 1
		\While{j >= 0 et tableau[j] > clé}
		\State tableau[j + 1] = tableau[j]
		\State j = j - 1
		\EndWhile
		\State tableau[j + 1] = clé
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Temps exponentiel (O($2^n$))}

Les algorithmes à temps exponentiel ont une complexité qui augmente de manière exponentielle avec la taille de l'entrée. Par exemple, un algorithme récursif pour résoudre le problème de la tour de Hanoï a une complexité `O($2^n$)`, car chaque mouvement peut générer de multiples autres mouvements.

\begin{algorithm}
	\caption{Tour de Hanoï}
	\begin{algorithmic}[1]
		\Function{hanoi}{n, source, cible, intermédiaire}
		\If{n == 1}
		\State \textbf{Déplacer} un disque de la source vers la cible
		\Else
		\State \Call{hanoi}{n - 1, source, intermédiaire, cible}
		\State \textbf{Déplacer} un disque de la source vers la cible
		\State \Call{hanoi}{n - 1, intermédiaire, cible, source}
		\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}


%La complexité des algorithmes varie selon le nombre d'opérations effectuées, le type d'opérations, et la structure de l'algorithme. Examinons pourquoi certains algorithmes ont des complexités spécifiques et comment ces complexités sont dérivées.
%
%\begin{itemize}
%	\item \textbf{Tri par insertion (Insertion Sort)} : La complexité de cet algorithme est généralement `O($n^2$)`. Cela provient du fait que, pour chaque élément du tableau, l'algorithme doit le comparer avec tous les éléments précédents pour l'insérer au bon endroit. Ainsi, pour un tableau de taille \(n\), il y a jusqu'à \(n - 1\) comparaisons pour chaque élément, menant à une complexité quadratique.
%	
%	\item \textbf{Tri rapide (QuickSort)} : La complexité de cet algorithme est `O(n log n)`, ce qui le rend plus efficace que le tri par insertion pour des tableaux de grande taille. Cela provient de sa stratégie de division et conquête, où le tableau est divisé en sous-tableaux autour d'un pivot, puis chaque sous-tableau est trié séparément. À chaque division, le problème est réduit de moitié, ce qui génère une complexité logarithmique. Comme chaque élément doit être comparé au pivot, cela crée la partie linéaire de la complexité.
%	
%	\item \textbf{Recherche linéaire (Linear Search)} : La complexité de cet algorithme est `O(n)`. Cela signifie que pour trouver un élément, l'algorithme doit potentiellement parcourir tous les éléments du tableau. Si l'élément recherché est le dernier, tous les éléments précédents doivent être examinés, d'où la complexité linéaire.
%	
%	\item \textbf{Recherche binaire (Binary Search)} : La complexité de cet algorithme est `O(log n)`, ce qui le rend plus rapide que la recherche linéaire pour des tableaux triés. Cette complexité provient du fait que l'algorithme divise le tableau en deux à chaque itération. Ainsi, avec chaque division, le problème est réduit de moitié, ce qui conduit à une complexité logarithmique.
%	
%	\item \textbf{Tri par sélection (Selection Sort)} : Cet algorithme a également une complexité de `O($n^2$)`. Dans le tri par sélection, l'algorithme trouve le plus petit élément du tableau et le place au début, puis répète ce processus pour les éléments suivants. Le nombre de comparaisons nécessaires pour chaque élément est lié à la taille du tableau, ce qui explique la complexité quadratique.
%\end{itemize}
